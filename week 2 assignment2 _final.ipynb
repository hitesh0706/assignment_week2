{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############cnn with batch normalization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#####importing data\n",
    "from sklearn.datasets import fetch_mldata #import data\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "A_prev = mnist.data\n",
    "Y = mnist.target\n",
    "A_prev = A_prev[0:1000,:]\n",
    "Y = np.array(Y)\n",
    "Y = Y[0:1000]\n",
    "Y = Y.reshape(1000,1)\n",
    "Y = Y.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########support functions\n",
    "\n",
    "def softmax(Z):\n",
    "    ps = np.exp(Z)\n",
    "    ps = ps/float(np.sum(ps))\n",
    "    return ps\n",
    "\n",
    "def relu(Z):\n",
    "    return np.multiply(Z,Z>0)\n",
    "\n",
    "\n",
    "def softmax_backprop (dA , A_prev):\n",
    "    avg = np.sum(A_prev,axis = 0)\n",
    "    numrtr = A_prev.dot(avg - A_prev)\n",
    "    dnmntr = np.square(avg)\n",
    "    dA_prev = np.multiply(dA,np.divide(numrtr,dnmntr))\n",
    "    return dA_prev\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flatten_a_data(X):\n",
    "    (m,a,b,c) = X.shape #no of training example\n",
    "    X = np.reshape(X,(m,a*b*c))\n",
    "    return X\n",
    "\n",
    "def loss(A,Y):\n",
    "    (m,n) = Y.shape\n",
    "    return -np.sum(np.multiply(np.log(A),Y))/m\n",
    "\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######convolution and pooling functions\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \n",
    "    X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values = (0,0))\n",
    "    \n",
    "    return X_pad\n",
    "\n",
    "\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "\n",
    "  \n",
    "    \n",
    "    s = np.multiply(a_slice_prev,W)\n",
    "  \n",
    "    Z = np.sum(s,axis=None)\n",
    "  \n",
    "    Z = Z + float(b)\n",
    "   \n",
    "\n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \n",
    "    \n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "   \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume\n",
    "    n_H = int((n_H_prev-f+2*pad)/stride)  +1\n",
    "    n_W = int((n_W_prev-f+2*pad)/stride) +1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "    Z = np.zeros((m,n_H,n_W,n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "    \n",
    "    for i in range(m):                               # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]                   # Select ith training example's padded activation\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = h*stride+f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = w*stride+f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad \n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end ,horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev,W[:,:,:,c],b[:,:,:,c])\n",
    "                                        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "   \n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = h*stride+f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = w*stride+f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i,vert_start:vert_end ,horiz_start:horiz_end,c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. \n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \n",
    "   \n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db =np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev,pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]\n",
    "        da_prev_pad = dA_prev_pad[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = h*stride + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = w*stride + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters \n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:,pad:,:]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = np.full((x.shape),False,dtype=bool)\n",
    "    ind = np.unravel_index(np.argmax(x, axis=None), x.shape)\n",
    "    mask[ind]=True\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = h*stride + f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = w*stride + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_pre\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) \n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(mask,dA[i,h,w,c])\n",
    "                        \n",
    "                   \n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, eps):  ####taken help from blog\n",
    "    N, D = x.shape\n",
    "\n",
    "  #step1: calculate mean\n",
    "    mu = 1./N * np.sum(x, axis = 0)\n",
    "\n",
    "  #step2: subtract mean vector of every trainings example\n",
    "    xmu = x - mu\n",
    "\n",
    "  #step3: following the lower branch - calculation denominator\n",
    "    sq = xmu ** 2\n",
    "\n",
    "  #step4: calculate variance\n",
    "    var = 1./N * np.sum(sq, axis = 0)\n",
    "\n",
    "  #step5: add eps for numerical stability, then sqrt\n",
    "    sqrtvar = np.sqrt(var + eps)\n",
    "\n",
    "  #step6: invert sqrtwar\n",
    "    ivar = 1./sqrtvar\n",
    "\n",
    "  #step7: execute normalization\n",
    "    xhat = xmu * ivar\n",
    "\n",
    "  #step8: Nor the two transformation steps\n",
    "    gammax = gamma * xhat\n",
    "\n",
    "  #step9\n",
    "    out = gammax + beta\n",
    "\n",
    "  #store intermediate\n",
    "    cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_backward(dout, cache):\n",
    "    xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
    "    #get the dimensions of the input/output\n",
    "    N,D = dout.shape\n",
    "\n",
    "  #step9\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgammax = dout #not necessary, but more understandable\n",
    "\n",
    "  #step8\n",
    "    dgamma = np.sum(dgammax*xhat, axis=0)\n",
    "    dxhat = dgammax * gamma\n",
    "\n",
    "  #step7\n",
    "    divar = np.sum(dxhat*xmu, axis=0)\n",
    "    dxmu1 = dxhat * ivar\n",
    "\n",
    "  #step6\n",
    "    dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "\n",
    "  #step5\n",
    "    dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "\n",
    "  #step4\n",
    "    dsq = 1. /N * np.ones((N,D)) * dvar\n",
    "\n",
    "  #step3\n",
    "    dxmu2 = 2 * xmu * dsq\n",
    "\n",
    "  #step2\n",
    "    dx1 = (dxmu1 + dxmu2)\n",
    "    dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
    "\n",
    "  #step1\n",
    "    dx2 = 1. /N * np.ones((N,D)) * dmu\n",
    "\n",
    "  #step0\n",
    "    dx = dx1 + dx2\n",
    "\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "###y one hot encoding\n",
    "Y = convert_to_one_hot(Y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape (A_prev):            #reshape input to (m,n_H,n_W,n_C)\n",
    "    X_layer = list()\n",
    "    for m in (A_prev):\n",
    "        X_layer.append(m.reshape(28,28))\n",
    "    X_layer = np.array(X_layer)\n",
    "    X_layer.shape  \n",
    "    X_layer = X_layer.reshape(X_layer.shape[0],X_layer.shape[1],X_layer.shape[2],1)\n",
    "    X_layer.shape\n",
    "    return X_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prev = reshape(A_prev)\n",
    "(m,n_H,n_W,n_C_prev)=A_prev.shape\n",
    "f = 3                           # no. of filters\n",
    "n_C_prev = 1                    #grey scale image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(f,f,n_C_prev,10)*0.01   #initialise filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.randn(1,1,1,10)*0.01       #initialise biase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparameters_conv = {\"pad\" : 0,          #initialise hparameters for convolution\n",
    "               \"stride\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparameters_pool = {\"f\" : 3,            #initialise hparameters for pooling\n",
    "                    \"stride\" : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc = np.random.randn(10,490)*0.01     #initialise weights for FC layer\n",
    "b_fc = np.random.rand(1000,10)*0.01     #initialise bias for biase\n",
    "lr = 0.01                               #learning rate \n",
    "epochs = 10                     #No. of epochs\n",
    "cost = np.zeros(epochs  )   \n",
    "\n",
    "#Initialise\n",
    "gamma = 1\n",
    "beta = 0\n",
    "epsilon = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.656730293194382\n",
      "9.875061846646274\n",
      "7.24498344836504\n",
      "7.072959568476159\n",
      "7.240604312362037\n",
      "7.070639604557852\n",
      "7.234723573847317\n",
      "7.068377173592964\n",
      "7.228983791333231\n",
      "7.0661248779513715\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    A, cache_conv = conv_forward(A_prev, W, b, hparameters_conv) #compute convolution\n",
    "    A1 ,cache_pool = pool_forward(A,hparameters_pool,mode = \"max\") #compute pooling\n",
    "    A2 = flatten_a_data(A1) #convert to fc layer\n",
    "    Z_fc = A2.dot(W_fc.T) +b_fc \n",
    "    Z_batch_normalize,cache_norm = batchnorm_forward(Z_fc,gamma,beta,epsilon)\n",
    "\n",
    "    A3 = softmax(Z_batch_normalize)              #apply softmax\n",
    "    cost[i] = loss(A3,Y)            #compute cost\n",
    "    dZ_fc = A3-Y  \n",
    "    d_batch,dgamaa,dbeta = batchnorm_backward(dZ_fc, cache_norm)\n",
    "    dW_fc = d_batch.T.dot(A2)\n",
    "    db_fc = dZ_fc\n",
    "    \n",
    "    dA2 = np.dot(dZ_fc,W_fc)\n",
    "   \n",
    "   \n",
    "    \n",
    "    \n",
    "    dA = pool_backward(A1 , cache_pool,mode = \"max\" )\n",
    "    dA_prev , dW , db = conv_backward(dA,cache_conv)\n",
    "    \n",
    "    W_fc = W_fc - dW_fc*lr\n",
    "    b_fc = b_fc - db_fc*lr\n",
    "    A2 = A2 - dA2*lr\n",
    "    A = A - dA*lr\n",
    "    W = W - dW*lr\n",
    "    b = b - db*lr\n",
    "    gamma = gamma - lr*dgamaa\n",
    "    beta = beta - lr*dbeta\n",
    "    A1 = np.reshape(A2,(1000,7,7,10))\n",
    "    print (cost[i])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6cea04c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt      \n",
    "%matplotlib inline \n",
    "# plot Cost vs no. of iterarions\n",
    "plt.plot(np.arange(len(cost)),cost)\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########accuracy\n",
    "\n",
    "\n",
    "\n",
    "#finallll_prediction = np.array(total_predictions[-100:])\n",
    "#output_original = X_label[0:100]\n",
    "\n",
    "#test_confusion = confusion_matrix(output_original,finallll_prediction)\n",
    "#test_confusion.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1000]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "final_pred = []\n",
    "A3\n",
    "for m in range(A3.shape[0]) :\n",
    "    final_pred.append(A3[m].argmax())\n",
    "final_pred =  np.array(final_pred)\n",
    "\n",
    "###output\n",
    "Y_output = mnist.target\n",
    "\n",
    "Y_output = np.array(Y_output)\n",
    "Y_output = Y_output[0:1000]\n",
    "Y_output = Y_output.reshape(1000,1)\n",
    "\n",
    "test_confusion = confusion_matrix(Y_output,final_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "p = np.zeros((A3.shape))\n",
    "count = 0\n",
    "for i in range (1000):\n",
    "   j = np.unravel_index(A3[i].argmax(), A3[i].shape)\n",
    "   if Y[i][j] == 1. :\n",
    "       count = count + 1\n",
    "accuracy = count / float(1000)\n",
    "print (accuracy*100)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
